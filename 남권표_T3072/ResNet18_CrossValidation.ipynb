{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058817d6-7199-432f-a8a7-9c36dbb67ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20754a5-b9a7-41cd-993f-ce2c9d56a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./image_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c153838-4079-43eb-804b-32d12011da52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               path  label\n",
       "0           0  ../input/data/train/images/000001_female_Asian...     10\n",
       "1           1  ../input/data/train/images/000001_female_Asian...      4\n",
       "2           2  ../input/data/train/images/000001_female_Asian...      4\n",
       "3           3  ../input/data/train/images/000001_female_Asian...      4\n",
       "4           4  ../input/data/train/images/000001_female_Asian...      4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4dc4b7d-7f46-4990-b86d-7e6bd5fa20a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(train_df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36272fb-6f14-4ef2-9245-3cacb01c8513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.sort(train_df['label'].unique()), train_df['label'].value_counts().sort_index())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78211cf2-4359-4b3a-af35-316b4d61a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1)\n",
    "if device == 'cuda': torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8e69b2-efde-4b7e-bd3a-0d340927c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSet(Dataset):\n",
    "    def __init__(self, train_image, train_label, transform):\n",
    "        self.train_image = train_image.tolist()\n",
    "        self.train_label = train_label.tolist()\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.train_label)\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.train_image[index])\n",
    "        label = torch.tensor(self.train_label[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0279712-09fe-45ea-9123-b8c536c46c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask, not 60 -> label: 0, 1, 3, 4\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd287c8d-5db9-4d00-aa15-07fec96ae24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = KFold(n_splits=5, shuffle=False)\n",
    "fold_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8598dce3-853e-4c73-aacf-964a115467c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, dataloader, criterion, optimizer):\n",
    "    train_loss, train_correct = 0, 0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()                                 # optimizer의 모든 파라미터들의 grad를 초기화한다.\n",
    "        hypothesis = model(images)                            # images를 model에 넣어주고 예측한다.\n",
    "        loss = criterion(hypothesis, labels)                    # loss값을 구한다.\n",
    "        loss.backward()                                       # loss값에 대해 미분을 수행한다.\n",
    "        optimizer.step()                                      # 학습을 진행한다. (optimizer를 한 단계 수행한다.)\n",
    "        train_loss += loss.item() * images.size(0)            # loss값에 image의 개수를 곱하고 저장한다.\n",
    "        scores, predictions = torch.max(hypothesis.data, 1)   # max로 가장 높게 예측한 값의 인덱스와 값(정수)를 뽑아준다.\n",
    "        train_correct += (predictions == labels).sum().item() # 인덱스가 label과 맞는지 확인하고 합을 구해서 맞힌 개수를 저장한다.\n",
    "  \n",
    "    return train_loss, train_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "667811fb-b554-46ba-90ba-ec630e399b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, device, dataloader, criterion):\n",
    "  valid_loss, val_correct = 0.0, 0\n",
    "  model.eval()\n",
    "  for images, labels in dataloader:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      prediction = model(images)\n",
    "      loss=criterion(prediction,labels)\n",
    "      valid_loss+=loss.item()*images.size(0)\n",
    "      scores, predictions = torch.max(prediction.data,1)\n",
    "      val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "  return valid_loss,val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f24cdb3-ade2-4c62-a814-56074b9ada92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', model_name = None, fold=None):\n",
    "        self.fold = fold\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.patience = patience                    # 학습이 개선되지 않을 경우 참을 횟수\n",
    "        self.verbose = verbose                      # 학습 개선 메세지를 출력할지 여부 결정\n",
    "        self.counter = 0                            # 학습이 개선되지 않은 횟수\n",
    "        self.best_score = None                      # 지금까지 가장 최적의 loss\n",
    "        self.early_stop = False                     # early_stop을 해야하는 경우 True로 값 변경\n",
    "        self.val_loss_min = np.Inf                  \n",
    "        self.delta = delta                          # 최소한의 loss값 개선 수준\n",
    "        self.path = path                            # 모델을 저장할 주소\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss                           # loss값을 음수로 바꾼다.\n",
    "\n",
    "        if self.best_score is None:                 # best_score에 score가 저장된 적이 없다면 \n",
    "            self.best_score = score                 # 해당값을 best_score에 저장하고\n",
    "            self.save_checkpoint(val_loss, model)   # loss값과 model을 저장한다.\n",
    "        elif score < self.best_score + self.delta:  # best_score에 delta를 더한 값보다 score가 작다면 == 이전 최대 점수에 delta를 뺀 값보다 score가 크다면 (delta값은 어느 정도의 학습을 강제하는 느낌인 것 같다.)\n",
    "            self.counter += 1                       # counter값을 갱신하고\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:       # counter값이 patience보다 크다면\n",
    "                self.early_stop = True              # early_stop을 True로 갱신한다.\n",
    "        else:\n",
    "            self.best_score = score                 # 만약 loss값이 알맞게 감소했다면\n",
    "            self.save_checkpoint(val_loss, model)   # loss값과 model을 저장한다.\n",
    "            self.counter = 0                        # counter값도 다시 0으로 초기화한다.\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        if self.fold is not None and self.model_name is not None:\n",
    "            torch.save(model.state_dict(), self.model_name + '_'+ str(self.fold)+'fold_'+self.path)\n",
    "        else:\n",
    "            torch.save(model.state_dict(),self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9c5ef9b-305b-477a-ae0b-5dc28f34c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "[Epoch:1/30] AVG Training Loss/Acc: 0.427/85.79, AVG Test Loss/Acc: 1.197/70.03\n",
      "[Epoch:2/30] AVG Training Loss/Acc: 0.157/94.42, AVG Test Loss/Acc: 1.907/54.60\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:3/30] AVG Training Loss/Acc: 0.138/95.15, AVG Test Loss/Acc: 1.207/73.73\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch:4/30] AVG Training Loss/Acc: 0.063/97.99, AVG Test Loss/Acc: 1.100/78.12\n",
      "[Epoch:5/30] AVG Training Loss/Acc: 0.032/99.11, AVG Test Loss/Acc: 0.827/77.96\n",
      "[Epoch:6/30] AVG Training Loss/Acc: 0.018/99.40, AVG Test Loss/Acc: 1.093/79.13\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:7/30] AVG Training Loss/Acc: 0.025/99.19, AVG Test Loss/Acc: 1.286/74.76\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch:8/30] AVG Training Loss/Acc: 0.047/98.47, AVG Test Loss/Acc: 1.101/79.10\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch:9/30] AVG Training Loss/Acc: 0.040/98.65, AVG Test Loss/Acc: 1.181/76.08\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[Epoch:10/30] AVG Training Loss/Acc: 0.066/97.78, AVG Test Loss/Acc: 1.157/77.09\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Fold 2\n",
      "[Epoch:1/30] AVG Training Loss/Acc: 0.413/86.81, AVG Test Loss/Acc: 1.041/72.25\n",
      "[Epoch:2/30] AVG Training Loss/Acc: 0.153/94.82, AVG Test Loss/Acc: 1.800/58.17\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:3/30] AVG Training Loss/Acc: 0.076/97.49, AVG Test Loss/Acc: 0.679/81.40\n",
      "[Epoch:4/30] AVG Training Loss/Acc: 0.047/98.40, AVG Test Loss/Acc: 1.179/70.90\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:5/30] AVG Training Loss/Acc: 0.068/97.66, AVG Test Loss/Acc: 0.780/79.68\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch:6/30] AVG Training Loss/Acc: 0.046/98.47, AVG Test Loss/Acc: 1.065/77.35\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch:7/30] AVG Training Loss/Acc: 0.037/98.90, AVG Test Loss/Acc: 0.732/83.39\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[Epoch:8/30] AVG Training Loss/Acc: 0.014/99.60, AVG Test Loss/Acc: 0.718/82.67\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Fold 3\n",
      "[Epoch:1/30] AVG Training Loss/Acc: 0.431/86.16, AVG Test Loss/Acc: 2.333/39.87\n",
      "[Epoch:2/30] AVG Training Loss/Acc: 0.163/94.58, AVG Test Loss/Acc: 0.783/74.52\n",
      "[Epoch:3/30] AVG Training Loss/Acc: 0.088/97.26, AVG Test Loss/Acc: 0.811/85.29\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:4/30] AVG Training Loss/Acc: 0.068/97.69, AVG Test Loss/Acc: 0.751/84.44\n",
      "[Epoch:5/30] AVG Training Loss/Acc: 0.026/99.18, AVG Test Loss/Acc: 0.740/87.46\n",
      "[Epoch:6/30] AVG Training Loss/Acc: 0.033/98.80, AVG Test Loss/Acc: 1.749/68.70\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:7/30] AVG Training Loss/Acc: 0.087/97.16, AVG Test Loss/Acc: 0.971/78.41\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch:8/30] AVG Training Loss/Acc: 0.062/97.94, AVG Test Loss/Acc: 2.245/50.37\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch:9/30] AVG Training Loss/Acc: 0.053/98.43, AVG Test Loss/Acc: 0.594/87.51\n",
      "[Epoch:10/30] AVG Training Loss/Acc: 0.022/99.41, AVG Test Loss/Acc: 0.631/88.15\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:11/30] AVG Training Loss/Acc: 0.019/99.34, AVG Test Loss/Acc: 0.825/86.06\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch:12/30] AVG Training Loss/Acc: 0.007/99.81, AVG Test Loss/Acc: 0.836/83.47\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch:13/30] AVG Training Loss/Acc: 0.004/99.89, AVG Test Loss/Acc: 0.908/84.58\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[Epoch:14/30] AVG Training Loss/Acc: 0.043/98.49, AVG Test Loss/Acc: 0.938/82.20\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Fold 4\n",
      "[Epoch:1/30] AVG Training Loss/Acc: 0.362/88.44, AVG Test Loss/Acc: 1.247/64.44\n",
      "[Epoch:2/30] AVG Training Loss/Acc: 0.142/95.35, AVG Test Loss/Acc: 1.290/64.97\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:3/30] AVG Training Loss/Acc: 0.095/96.80, AVG Test Loss/Acc: 1.454/65.37\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch:4/30] AVG Training Loss/Acc: 0.032/99.03, AVG Test Loss/Acc: 1.791/68.86\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch:5/30] AVG Training Loss/Acc: 0.066/97.84, AVG Test Loss/Acc: 1.905/59.39\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[Epoch:6/30] AVG Training Loss/Acc: 0.034/98.92, AVG Test Loss/Acc: 2.703/54.60\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Fold 5\n",
      "[Epoch:1/30] AVG Training Loss/Acc: 0.416/86.08, AVG Test Loss/Acc: 1.073/67.38\n",
      "[Epoch:2/30] AVG Training Loss/Acc: 0.171/94.07, AVG Test Loss/Acc: 0.764/79.13\n",
      "[Epoch:3/30] AVG Training Loss/Acc: 0.067/98.00, AVG Test Loss/Acc: 0.794/79.95\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:4/30] AVG Training Loss/Acc: 0.056/98.21, AVG Test Loss/Acc: 1.184/72.20\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch:5/30] AVG Training Loss/Acc: 0.117/96.06, AVG Test Loss/Acc: 1.170/70.63\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch:6/30] AVG Training Loss/Acc: 0.028/99.12, AVG Test Loss/Acc: 0.704/86.43\n",
      "[Epoch:7/30] AVG Training Loss/Acc: 0.019/99.38, AVG Test Loss/Acc: 0.637/87.25\n",
      "[Epoch:8/30] AVG Training Loss/Acc: 0.022/99.33, AVG Test Loss/Acc: 0.777/84.87\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch:9/30] AVG Training Loss/Acc: 0.042/98.66, AVG Test Loss/Acc: 1.646/68.31\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch:10/30] AVG Training Loss/Acc: 0.030/99.09, AVG Test Loss/Acc: 1.720/61.88\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch:11/30] AVG Training Loss/Acc: 0.025/99.15, AVG Test Loss/Acc: 0.885/81.93\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[Epoch:12/30] AVG Training Loss/Acc: 0.040/98.71, AVG Test Loss/Acc: 1.278/71.93\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrainSet(train_df['path'], train_df['label'], transform)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=False, fold=fold, model_name = 'ResNet18_noShuffle')\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = timm.create_model('resnet18', pretrained=True, num_classes=18).to(device)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    history = {'train_loss': [], 'valid_loss': [],'train_acc':[],'valid_acc':[]}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n",
    "        valid_loss, val_correct=valid_epoch(model,device,valid_loader,criterion)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "        valid_acc = val_correct / len(valid_loader.sampler) * 100\n",
    "\n",
    "        print(f\"[Epoch:{epoch+1}/{num_epochs}] AVG Training Loss/Acc: {train_loss:.3f}/{train_acc:.2f}, AVG Test Loss/Acc: {valid_loss:.3f}/{valid_acc:.2f}\")\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['valid_loss'].append(valid_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['valid_acc'].append(valid_acc)\n",
    "        \n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    fold_dict['fold{}'.format(fold+1)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bb1e725-bd75-4c44-b69c-34dfeea589a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "dataset = TestDataset(image_paths, transform_test)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model1 = timm.create_model('resnet18', pretrained=True, num_classes=18).to(device); model1.load_state_dict(torch.load('./ResNet_noShuffle_noAug/0fold_checkpoint.pt'))\n",
    "model2 = timm.create_model('resnet18', pretrained=True, num_classes=18).to(device); model2.load_state_dict(torch.load('./ResNet_noShuffle_noAug/1fold_checkpoint.pt'))\n",
    "model3 = timm.create_model('resnet18', pretrained=True, num_classes=18).to(device); model3.load_state_dict(torch.load('./ResNet_noShuffle_noAug/2fold_checkpoint.pt'))\n",
    "model4 = timm.create_model('resnet18', pretrained=True, num_classes=18).to(device); model4.load_state_dict(torch.load('./ResNet_noShuffle_noAug/3fold_checkpoint.pt'))\n",
    "model5 = timm.create_model('resnet18', pretrained=True, num_classes=18).to(device); model5.load_state_dict(torch.load('./ResNet_noShuffle_noAug/4fold_checkpoint.pt'))\n",
    "models = [model1, model2, model3, model4, model5]\n",
    "\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        pred = 0\n",
    "        images = images.to(device)\n",
    "        for model in models:\n",
    "            pred += model(images) / len(models) \n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
